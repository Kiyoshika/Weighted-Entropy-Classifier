{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0df48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_blobs, make_moons, make_circles\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9305a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_classification(n_features = 100, n_samples = 2000, n_informative = 2, n_redundant = 2, class_sep = 0.3, weights=[0.15, 0.85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8a6fd344",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(data[0], data[1], test_size = 0.2)\n",
    "xtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "23bda40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_res, ytrain_res = SMOTE(sampling_strategy = 'minority').fit_resample(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f4e841ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  47],\n",
       "       [ 13, 339]], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytest, LogisticRegression(max_iter=1000).fit(xtrain, ytrain).predict(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "aecd156f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 20,  28],\n",
       "       [117, 235]], dtype=int64)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytest, LogisticRegression(max_iter=1000).fit(xtrain_res, ytrain_res).predict(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cee41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c1ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ecfbe963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18,  30],\n",
       "       [ 55, 297]], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytest, GaussianNB().fit(xtrain, ytrain).predict(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1088d5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14,  34],\n",
       "       [ 54, 298]], dtype=int64)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytest, GaussianNB().fit(xtrain_res, ytrain_res).predict(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b91706b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, entropy, gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7e503042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedEntropyClassifier:\n",
    "    minority_class = None\n",
    "    majority_class = None\n",
    "    use_normal = False\n",
    "    proportions = []\n",
    "    \n",
    "    def __init__(self, use_normal = False):\n",
    "        self.use_normal = use_normal\n",
    "        \n",
    "    # get entropy weights (e.g, \"fitting\" the model)\n",
    "    def fit(self, xtrain, ytrain, use_normal = False):\n",
    "        full_train = pd.DataFrame(np.concatenate((xtrain, ytrain.reshape(-1,1)), axis=1))\n",
    "        full_train.columns = [*full_train.columns[:-1], 'target']\n",
    "        self.minority_class = full_train[full_train.target == 0]\n",
    "        self.majority_class = full_train[full_train.target == 1]\n",
    "\n",
    "        feature_max_entropy = []\n",
    "        # clear proportions vector to \"refit\" model (if already fitted)\n",
    "        self.proportions = []\n",
    "\n",
    "        for i in range(xtrain.shape[1]):\n",
    "            current_feature = i\n",
    "            points = np.linspace(min(min(self.minority_class.iloc[:,current_feature]), min(self.majority_class.iloc[:,current_feature])), max(max(self.minority_class.iloc[:,current_feature]), max(self.majority_class.iloc[:,current_feature])), 50)\n",
    "            if self.use_normal == True:\n",
    "                min_pdf = norm.pdf(points, loc = np.mean(self.minority_class.iloc[:,current_feature]), scale = np.std(self.minority_class.iloc[:,current_feature]))\n",
    "                maj_pdf = norm.pdf(points, loc = np.mean(self.majority_class.iloc[:,current_feature]), scale = np.std(self.majority_class.iloc[:,current_feature]))\n",
    "            else:\n",
    "                min_pdf = gaussian_kde(self.minority_class.iloc[:,current_feature]).evaluate(points)\n",
    "                maj_pdf = gaussian_kde(self.majority_class.iloc[:,current_feature]).evaluate(points)\n",
    "                # relative entropy is not commutative\n",
    "                entrp =  max(entropy(min_pdf, maj_pdf), entropy(maj_pdf, min_pdf))\n",
    "                feature_max_entropy.append(entrp)\n",
    "\n",
    "        for fme in feature_max_entropy:\n",
    "                self.proportions.append(fme / sum(feature_max_entropy))\n",
    "\n",
    "    # compare new data points with weighted entropies and likelihood\n",
    "    def predict(self, xtest, min_bias = 0, maj_bias = 0):\n",
    "        preds = []\n",
    "        for i in range(xtest.shape[0]):\n",
    "            min_score = 0\n",
    "            maj_score = 0\n",
    "            for j in range(len(self.proportions)):\n",
    "                data_point = xtest[i][j]\n",
    "                if self.use_normal == True:\n",
    "                    min_score += self.proportions[j]*norm.pdf(data_point, loc = np.mean(self.minority_class.iloc[:,j]), scale = np.std(self.minority_class.iloc[:,j]))\n",
    "                    maj_score += self.proportions[j]*norm.pdf(data_point, loc = np.mean(self.majority_class.iloc[:,j]), scale = np.std(self.majority_class.iloc[:,j]))\n",
    "                else:\n",
    "                    min_score += self.proportions[j]*gaussian_kde(self.minority_class.iloc[:,j]).evaluate(data_point)\n",
    "                    maj_score += self.proportions[j]*gaussian_kde(self.majority_class.iloc[:,j]).evaluate(data_point)\n",
    "            preds.append(np.argmax([min_score + min_bias, maj_score + maj_bias]))\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d1ea0e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wec = WeightedEntropyClassifier()\n",
    "wec.fit(xtrain[:,selected_feats], ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "00a93897",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypreds = wec.predict(xtest[:,selected_feats], min_bias = 0.35, maj_bias = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "397158c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33,  15],\n",
       "       [170, 182]], dtype=int64)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytest, mypreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ece464a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total selected features:  6\n"
     ]
    }
   ],
   "source": [
    "selected_feats = []\n",
    "for i in range(len(wec.proportions)):\n",
    "    if wec.proportions[i] >= 0.003:\n",
    "        selected_feats.append(i)\n",
    "        \n",
    "print(\"Total selected features: \", len(selected_feats))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
